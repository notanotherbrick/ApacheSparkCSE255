{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "textRDD = sc.newAPIHadoopFile('data/Moby-Dick.txt',\n",
    "                              'org.apache.hadoop.mapreduce.lib.input.TextInputFormat',\n",
    "                              'org.apache.hadoop.io.LongWritable',\n",
    "                              'org.apache.hadoop.io.Text',\n",
    "                              conf={'textinputformat.record.delimiter': \"\\r\\n\\r\\n\"}) \\\n",
    ".map(lambda x: x[1])\n",
    "\n",
    "sentences=textRDD.map(lambda x:x.replace('\\r\\n',' '))\\\n",
    "        .flatMap(lambda x: x.split(\". \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Function for cleaning punctuation\n",
    "def cleanData(inputstr):\n",
    "    s=list(inputstr) # convert string to list of characters\n",
    "    for i in range(len(s)):\n",
    "        o=ord(s[i])             # ASCII value of each string\n",
    "        if o==46 or (o>=48 and o<=57) or (o>=97 and o<=122) : #space, numbers and lowercase left as is\n",
    "            continue \n",
    "        elif (o>=65 and o<=90): # convert uppercase to lowercase\n",
    "            s[i]=chr(o+32)  \n",
    "        else:               # replace with space for anything else \n",
    "            s[i]=\" \" \n",
    "        \n",
    "    return str(\"\".join(s)) # str to convert unicode string to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences_rm_punct=sentences.map(cleanData)#sentences_rm_rn.map(cleanData) # clean data of punctuation, uppercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sentence2words=sentences_rm_punct.map(lambda x: x.split(\" \"))\\\n",
    "    .filter(lambda x: x != ['']) # split by space and remove blank sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def removeBlankWords(inputlist):\n",
    "    y=[]\n",
    "    for x in inputlist:\n",
    "        if x != '':\n",
    "            y.append(x)\n",
    "    return y\n",
    "sentence2words_clean=sentence2words.map(removeBlankWords) # remove blank words of form ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printOutput(n,freq_ngramRDD):\n",
    "    top=freq_ngramRDD.take(5)\n",
    "    print '\\n============ %d most frequent %d-grams'%(5,n)\n",
    "    print '\\nindex\\tcount\\tngram'\n",
    "    for i in range(5):\n",
    "        print '%d.\\t%d: \\t\"%s\"'%(i+1,top[i][0],' '.join(top[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============ 5 most frequent 1-grams\n",
      "\n",
      "index\tcount\tngram\n",
      "1.\t14620: \t\"the\"\n",
      "2.\t6731: \t\"of\"\n",
      "3.\t6502: \t\"and\"\n",
      "4.\t4785: \t\"a\"\n",
      "5.\t4702: \t\"to\"\n",
      "\n",
      "============ 5 most frequent 2-grams\n",
      "\n",
      "index\tcount\tngram\n",
      "1.\t1906: \t\"of the\"\n",
      "2.\t1193: \t\"in the\"\n",
      "3.\t746: \t\"to the\"\n",
      "4.\t444: \t\"from the\"\n",
      "5.\t404: \t\"the whale\"\n",
      "\n",
      "============ 5 most frequent 3-grams\n",
      "\n",
      "index\tcount\tngram\n",
      "1.\t114: \t\"the sperm whale\"\n",
      "2.\t102: \t\"of the whale\"\n",
      "3.\t88: \t\"the white whale\"\n",
      "4.\t64: \t\"one of the\"\n",
      "5.\t58: \t\"out of the\"\n",
      "\n",
      "============ 5 most frequent 4-grams\n",
      "\n",
      "index\tcount\tngram\n",
      "1.\t42: \t\"of the sperm whale\"\n",
      "2.\t27: \t\"the sperm whale s\"\n",
      "3.\t20: \t\"at the same time\"\n",
      "4.\t18: \t\"project gutenberg tm electronic\"\n",
      "5.\t18: \t\"of the whale s\"\n",
      "\n",
      "============ 5 most frequent 5-grams\n",
      "\n",
      "index\tcount\tngram\n",
      "1.\t13: \t\"the project gutenberg literary archive\"\n",
      "2.\t12: \t\"project gutenberg literary archive foundation\"\n",
      "3.\t11: \t\"project gutenberg tm electronic works\"\n",
      "4.\t11: \t\"of the sperm whale s\"\n",
      "5.\t10: \t\"and at the same time\"\n"
     ]
    }
   ],
   "source": [
    "for n in range(1,6):\n",
    "    if n==1:\n",
    "        pairs=sentence2words_clean.flatMap(lambda x:x)\n",
    "        freq_ngramRDD=pairs.map(lambda word: (word, 1))\\\n",
    "             .reduceByKey(lambda a, b: a + b)\\\n",
    "             .sortBy(lambda x:x[1],False)\\\n",
    "             .map(lambda x: (x[1],[x[0]]))\n",
    "    else:\n",
    "        if n==2:\n",
    "            pairs=sentence2words_clean.flatMap(lambda x:zip(x,x[1:])) # create bigram\n",
    "        elif n==3:\n",
    "            pairs=sentence2words_clean.flatMap(lambda x:zip(x,x[1:],x[2:])) # create trigram\n",
    "        elif n==4:\n",
    "            pairs=sentence2words_clean.flatMap(lambda x:zip(x,x[1:],x[2:],x[3:])) #create 4-grams\n",
    "        elif n==5:\n",
    "            pairs=sentence2words_clean.flatMap(lambda x:zip(x,x[1:],x[2:],x[3:],x[4:])) #create 5-grams\n",
    "\n",
    "        freq_ngramRDD=pairs.map(lambda word: (word, 1)) \\\n",
    "                 .reduceByKey(lambda a, b: a + b)\\\n",
    "                 .sortBy(lambda x:x[1],False)\\\n",
    "                 .map(lambda x: (x[1],x[0]))\n",
    "        \n",
    "    printOutput(n,freq_ngramRDD)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
